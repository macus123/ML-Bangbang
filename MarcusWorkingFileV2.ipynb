{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "from datetime import datetime, timezone\n",
    "import datetime\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import OPTICS\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "import copy\n",
    "from scipy.stats import mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveform(filepath):\n",
    "    with open(filepath) as dataFile:\n",
    "        data = dataFile.read()\n",
    "        # Check for missing commas and insert them\n",
    "        pattern = r'(\\{[^{}]*\"[^\"]*\"[^{}]*\\})(?=[^{}]*\\{)'\n",
    "        data = re.sub(pattern, r'\\1,', data)\n",
    "\n",
    "        obj = data[data.find('{') : data.rfind('}')+1]\n",
    "        jsonObj = json.loads(obj)\n",
    "        \n",
    "        if jsonObj['data']:\n",
    "            waveform_data = pd.DataFrame(jsonObj['data'], columns=[\"peak_amplitude\",\"integral\",\"phase_angle\",\"cycle_number\",\"rise_time\",\"pulse_width\"])    \n",
    "            return waveform_data\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(dirpath):\n",
    "    dataframes = []\n",
    "    for filename in os.listdir(dirpath):\n",
    "        if filename.endswith(\".js\"):\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            waveform = load_waveform(filepath)\n",
    "            if not waveform.empty:\n",
    "                waveform['filename'] = filename # Add filename as a column\n",
    "                dataframes.append(waveform)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "dirpath1 = \"ptest\"\n",
    "dirpath2 = \"ntest\"\n",
    "dirpath3 = \"utest\"\n",
    "pdf = load_dataframes(dirpath1)\n",
    "ndf = load_dataframes(dirpath2)\n",
    "udf = load_dataframes(dirpath3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_vars = {} # create a dictionary with the assigned dependent variables mapped to the filename as the key\n",
    "for df, y in zip([pdf, ndf, udf], [1, 2, 0]): # 1 is positive, 2 is negative, 0 is unclassified\n",
    "    for filename in pd.concat(df)['filename'].unique():\n",
    "        dependent_vars[filename] = y\n",
    "# this is used to assign the dependent variables to the filename in the transformed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all list of dataframes into a single list of dataframes once dependent variables have been assigned\n",
    "dataframes = pdf + ndf + udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframes) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dataframes: # show data into plots\n",
    "    plt.scatter(dataframe['phase_angle'], dataframe['peak_amplitude'], s=8)\n",
    "    plt.title(dataframe['filename'].iloc[0])\n",
    "    plt.xlabel('Phase angle')\n",
    "    plt.ylabel('Peak amplitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_baseline(bucket_data, kmeans, predicted_clusters, threshold=5): # algorithm to determine baselines\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    centroid_distance = abs(centroids[1] - centroids[0])\n",
    "\n",
    "    if centroid_distance > threshold:\n",
    "        # Clusters far apart - Outcome 1\n",
    "        lower_centroid_index = np.argmin(centroids)\n",
    "        lower_cluster = bucket_data[predicted_clusters == lower_centroid_index]\n",
    "        return max(lower_cluster['peak_amplitude'])\n",
    "    else:\n",
    "        # Clusters close together - Outcome 2\n",
    "        higher_centroid_index = np.argmax(centroids)\n",
    "        return centroids[higher_centroid_index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_dataframes(dataframes): # function to calculate base-lines and plot the graphs with the new added information\n",
    "    warnings.filterwarnings('ignore')\n",
    "    for df in dataframes:\n",
    "        # if df['peak_amplitude'].max() > 8: # filter out the negative\n",
    "        #     continue\n",
    "\n",
    "        # Bucket phase angles\n",
    "        df['bucket'] = pd.cut(df['phase_angle'], bins=range(0, 361, 10), labels=False)\n",
    "\n",
    "        # Perform k-means clustering on each bucket and determine baselines\n",
    "        kmeans_results = []\n",
    "        bucket_baselines = []\n",
    "        for i in range(36):\n",
    "            bucket_data = df[df['bucket'] == i]\n",
    "            if not bucket_data.empty and len(bucket_data) > 1:\n",
    "                kmeans = KMeans(n_clusters=2, random_state=0, n_init=10)\n",
    "                kmeans.fit(bucket_data['peak_amplitude'].values.reshape(-1, 1))\n",
    "                kmeans_results.append((i, kmeans))\n",
    "\n",
    "                # Predict the clusters for bucket_data\n",
    "                predicted_clusters = kmeans.predict(bucket_data['peak_amplitude'].values.reshape(-1, 1))\n",
    "\n",
    "                # Pass the predicted clusters to the determine_baseline function\n",
    "                baseline = determine_baseline(bucket_data, kmeans, predicted_clusters)\n",
    "                bucket_baselines.append((i, baseline))\n",
    "            else:\n",
    "                kmeans_results.append((i, None))\n",
    "                bucket_baselines.append((i, None))\n",
    "\n",
    "        # Assign cluster labels to each data point\n",
    "        df['cluster'] = np.nan\n",
    "        for i, kmeans in kmeans_results:\n",
    "            if kmeans is not None:\n",
    "                mask = df['bucket'] == i\n",
    "                df.loc[mask, 'cluster'] = kmeans.predict(df.loc[mask, 'peak_amplitude'].values.reshape(-1, 1))\n",
    "\n",
    "        baselines = [baseline for _, baseline in bucket_baselines if baseline is not None]\n",
    "        basemode = mode(baselines).mode[0]\n",
    "\n",
    "        # Remove points below the baseline mode (inserted)\n",
    "        df.loc[:, 'filtered'] = df['peak_amplitude'] >= basemode\n",
    "\n",
    "        # # Scatter plot with bucketed phase angles and clustered data points\n",
    "        # plt.scatter(df['phase_angle'], df['peak_amplitude'], c=df['cluster'], cmap='viridis', s=8)\n",
    "        # plt.title(df['filename'].iloc[0])\n",
    "        # plt.xlabel('Phase angle')\n",
    "        # plt.ylabel('Peak amplitude')\n",
    "\n",
    "        # # Plot baselines\n",
    "        # for i, baseline in bucket_baselines:\n",
    "        #     if baseline is not None:\n",
    "        #         plt.hlines(baseline, i * 10, (i + 1) * 10 - 1, colors='r', linestyles='dashed')\n",
    "        # plt.hlines(basemode, 0, 359, colors='b', linestyles='solid')\n",
    "        # plt.show()\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = denoise_dataframes(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframes[2]) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframes = [df[df['filtered'] == True] for df in dataframes] # remove all data points below the baseline AKA removing noise\n",
    "print(filtered_dataframes[1]) # check if remaining values are True, meaning that the data points remaining are the ones that we want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in filtered_dataframes: # show new graphs with threshold removed\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(dataframe['phase_angle'], dataframe['peak_amplitude'], s=8)\n",
    "    ax.set_title(dataframe['filename'].iloc[0])\n",
    "    ax.set_xlabel('Phase angle')\n",
    "    ax.set_ylabel('Peak amplitude')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for the cluster features\n",
    "for df in filtered_dataframes:\n",
    "    # Extract the relevant columns\n",
    "    X = df[['phase_angle', 'peak_amplitude']].values\n",
    "    \n",
    "    # Apply DBSCAN to the data\n",
    "    dbscan = DBSCAN(eps=7, min_samples=8)\n",
    "    dbscan.fit(X)\n",
    "    labels = dbscan.labels_\n",
    "    \n",
    "    # Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "    df_copy = df.copy()\n",
    "    df_copy['cluster_length'] = np.nan\n",
    "    df_copy['cluster_height'] = np.nan\n",
    "    df_copy['cluster_gradient_tr'] = np.nan\n",
    "    df_copy['cluster_gradient_tl'] = np.nan\n",
    "    \n",
    "    # Add the cluster labels to the dataframe copy\n",
    "    df_copy['cluster'] = labels\n",
    "    \n",
    "    # Access cluster information\n",
    "    clusters = set(labels)\n",
    "    for cluster in clusters:\n",
    "        if cluster != -1:\n",
    "            # Get the points belonging to the cluster\n",
    "            cluster_points = X[labels == cluster]\n",
    "\n",
    "            \n",
    "            # Calculate cluster features\n",
    "            cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "            cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "            cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "            cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "            \n",
    "            # Add new columns for cluster features to the dataframe copy\n",
    "            df_copy.loc[labels == cluster, 'cluster_length'] = cluster_length\n",
    "            df_copy.loc[labels == cluster, 'cluster_height'] = cluster_height\n",
    "            df_copy.loc[labels == cluster, 'cluster_gradient_tr'] = cluster_gradient_tr\n",
    "            df_copy.loc[labels == cluster, 'cluster_gradient_tl'] = cluster_gradient_tl\n",
    "    \n",
    "    # Assign the cluster labels and features to the original dataframe\n",
    "    df.loc[:, 'cluster'] = df_copy['cluster']\n",
    "    df.loc[:, 'cluster_length'] = df_copy['cluster_length']\n",
    "    df.loc[:, 'cluster_height'] = df_copy['cluster_height']\n",
    "    df.loc[:, 'cluster_gradient_tr'] = df_copy['cluster_gradient_tr']\n",
    "    df.loc[:, 'cluster_gradient_tl'] = df_copy['cluster_gradient_tl']\n",
    "    \n",
    "    # # Plot the clustered data\n",
    "    # plt.scatter(df['phase_angle'], df['peak_amplitude'], c=df['cluster'], cmap='viridis', s=8)\n",
    "    # plt.title(df['filename'].iloc[0])\n",
    "    # plt.xlabel('Phase angle')\n",
    "    # plt.ylabel('Peak amplitude')\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_cluster_bounds(cluster_points):\n",
    "#     x_min, x_max = cluster_points[:, 0].min(), cluster_points[:, 0].max()\n",
    "#     y_min, y_max = cluster_points[:, 1].min(), cluster_points[:, 1].max()\n",
    "#     plt.plot([x_min, x_max], [y_min, y_min], 'k-', linewidth=2)\n",
    "#     plt.plot([x_max, x_max], [y_min, y_max], 'k-', linewidth=2)\n",
    "#     plt.plot([x_max, x_min], [y_max, y_max], 'k-', linewidth=2)\n",
    "#     plt.plot([x_min, x_min], [y_max, y_min], 'k-', linewidth=2)\n",
    "\n",
    "\n",
    "# for df in filtered_dataframes:\n",
    "#     # Extract the relevant columns\n",
    "#     X = df[['phase_angle', 'peak_amplitude']].values\n",
    "    \n",
    "#     # Apply DBSCAN to the data\n",
    "#     dbscan = DBSCAN(eps=10, min_samples=10)\n",
    "#     dbscan.fit(X)\n",
    "#     labels = dbscan.labels_\n",
    "    \n",
    "#     # Add the cluster labels to the dataframe\n",
    "#     df['cluster'] = labels\n",
    "    \n",
    "#     # Create new columns for each cluster label\n",
    "#     dummies = pd.get_dummies(df['cluster'], prefix='cluster')\n",
    "#     df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "#     # Access cluster information\n",
    "#     clusters = set(labels)\n",
    "#     for cluster in clusters:\n",
    "#         if cluster != -1:\n",
    "#             # Get the points belonging to the cluster\n",
    "#             cluster_points = X[labels == cluster]\n",
    "            \n",
    "#             # Calculate cluster features\n",
    "#             cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "#             cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "#             cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "#             cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "            \n",
    "#             # Print cluster information\n",
    "#             print(f\"Cluster {cluster}: Length={cluster_length}, Height={cluster_height}, GradientTR={cluster_gradient_tr}, GradientTL={cluster_gradient_tl}\")\n",
    "            \n",
    "#             # Plot the boundaries of the cluster\n",
    "#             plot_cluster_bounds(cluster_points)\n",
    "    \n",
    "#     # Plot the clustered data\n",
    "#     plt.scatter(df['phase_angle'], df['peak_amplitude'], c=df['cluster'], cmap='viridis', s=8)\n",
    "#     plt.title(df['filename'].iloc[0])\n",
    "#     plt.xlabel('Phase angle')\n",
    "#     plt.ylabel('Peak amplitude')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_dataframes[0]) # integers in the cluster column indicate the cluster that they belong to, if its a -1 it means that it is an outlier and is not included in any clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once the relevant graphs have been removed, convert list of dataframes into 1 big dataframe\n",
    "big_df = pd.concat(filtered_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.isna().sum() # check number of empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.fillna(0, inplace=True) # replace missing values with 0\n",
    "big_df.isna().sum() # check again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate additional clustering features based on the cluster labels\n",
    "# clusters = []\n",
    "# for filename, file_df in big_df.groupby('filename'):\n",
    "#     # Assume the cluster labels are stored in the 'cluster' column\n",
    "#     labels = file_df['cluster'].values\n",
    "#     unique_labels = np.unique(labels)\n",
    "#     cluster_count = len(unique_labels)\n",
    "#     if cluster_count > 0:\n",
    "#         # Calculate additional clustering features based on the cluster labels\n",
    "#         # For example, cluster_length, cluster_height, etc.\n",
    "#         # Append the calculated features to the clusters list\n",
    "#         cluster_lengths = []\n",
    "#         cluster_heights = []\n",
    "#         cluster_gradient_trs = []\n",
    "#         cluster_gradient_tls = []\n",
    "#         for label in unique_labels:\n",
    "#             cluster_points = file_df[file_df['cluster'] == label][['phase_angle', 'peak_amplitude']].values\n",
    "#             if len(cluster_points) > 1:\n",
    "#                 cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "#                 cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "#                 cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "#                 cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "#                 cluster_lengths.append(cluster_length)\n",
    "#                 cluster_heights.append(cluster_height)\n",
    "#                 cluster_gradient_trs.append(cluster_gradient_tr)\n",
    "#                 cluster_gradient_tls.append(cluster_gradient_tl)\n",
    "#         clusters.append({'filename': filename, 'cluster_count': cluster_count, 'cluster_lengths': cluster_lengths, 'cluster_heights': cluster_heights, 'cluster_gradient_trs': cluster_gradient_trs, 'cluster_gradient_tls': cluster_gradient_tls})\n",
    "\n",
    "# # Convert the clusters list to a dataframe\n",
    "# clusters_df = pd.DataFrame(clusters)\n",
    "\n",
    "# final_df = clusters_df\n",
    "\n",
    "# final_df['anomaly'] = final_df['filename'].apply(lambda x: dependent_vars.get(x, 0)) # call dictionary with filename keys mapped to dependent variables respective to each file\n",
    "\n",
    "# print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional clustering features based on the cluster labels\n",
    "clusters = []\n",
    "for filename, file_df in big_df.groupby('filename'):\n",
    "    # Assume the cluster labels are stored in the 'cluster' column\n",
    "    labels = file_df['cluster'].values\n",
    "    unique_labels = np.unique(labels)\n",
    "    cluster_count = len(unique_labels)\n",
    "    if cluster_count > 0:\n",
    "        # Calculate additional clustering features based on the cluster labels\n",
    "        # For example, cluster_length, cluster_height, etc.\n",
    "        # Append the calculated features to the clusters list\n",
    "        cluster_lengths = []\n",
    "        cluster_heights = []\n",
    "        cluster_gradient_trs = []\n",
    "        cluster_gradient_tls = []\n",
    "        for i in range(4):\n",
    "            if i < len(unique_labels):\n",
    "                label = unique_labels[i]\n",
    "                cluster_points = file_df[file_df['cluster'] == label][['phase_angle', 'peak_amplitude']].values\n",
    "                if len(cluster_points) > 1:\n",
    "                    cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "                    cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "                    cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "                    cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "                    cluster_lengths.append(cluster_length)\n",
    "                    cluster_heights.append(cluster_height)\n",
    "                    cluster_gradient_trs.append(cluster_gradient_tr)\n",
    "                    cluster_gradient_tls.append(cluster_gradient_tl)\n",
    "            else:\n",
    "                # Fill in zeros for any missing clusters\n",
    "                cluster_lengths.append(0)\n",
    "                cluster_heights.append(0)\n",
    "                cluster_gradient_trs.append(0)\n",
    "                cluster_gradient_tls.append(0)\n",
    "        clusters.append({'filename': filename, 'cluster_count': cluster_count, 'cluster_lengths': cluster_lengths, 'cluster_heights': cluster_heights, 'cluster_gradient_trs': cluster_gradient_trs, 'cluster_gradient_tls': cluster_gradient_tls})\n",
    "\n",
    "# Convert the clusters list to a dataframe\n",
    "clusters_df = pd.DataFrame(clusters)\n",
    "\n",
    "final_df = clusters_df\n",
    "\n",
    "final_df['anomaly'] = final_df['filename'].apply(lambda x: dependent_vars.get(x, 0)) # call dictionary with filename keys mapped to dependent variables respective to each file\n",
    "\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('test1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df\n",
    "# Expand the list columns into separate columns\n",
    "expanded_df = pd.concat([df.drop(['cluster_lengths', 'cluster_heights', 'cluster_gradient_trs', 'cluster_gradient_tls'], axis=1),\n",
    "                         df['cluster_lengths'].apply(pd.Series).add_prefix('cluster_length_'),\n",
    "                         df['cluster_heights'].apply(pd.Series).add_prefix('cluster_height_'),\n",
    "                         df['cluster_gradient_trs'].apply(pd.Series).add_prefix('cluster_gradient_tr_'),\n",
    "                         df['cluster_gradient_tls'].apply(pd.Series).add_prefix('cluster_gradient_tl_')],\n",
    "                        axis=1)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "expanded_df.fillna(0, inplace=True)\n",
    "\n",
    "# be sure to include this in your model training code so that the filename and anomalies are not calculated as features!\n",
    "# Define the features and target variables:\n",
    "# features = expanded_df.columns.tolist()\n",
    "# features.remove('filename')\n",
    "# features.remove('anomaly')\n",
    "# target = 'anomaly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Split the data into training and testing sets\n",
    "train_dfs, test_dfs = train_test_split(expanded_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the features and target variables\n",
    "features = expanded_df.columns.tolist()\n",
    "features.remove('filename')\n",
    "features.remove('anomaly')\n",
    "target = 'anomaly'\n",
    "\n",
    "# Train the decision tree model\n",
    "DecisionTreeClassifierModel = DecisionTreeClassifier(random_state=42)\n",
    "DecisionTreeClassifierModel.fit(train_dfs[features], train_dfs[target])\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = DecisionTreeClassifierModel.predict(test_dfs[features])\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(test_dfs[target], predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_dfs[target], predictions)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_df)\n",
    "expanded_df.to_csv('test1.1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataframes):\n",
    "    dataframes = denoise_dataframes(dataframes) # de-noise the list of dataframes\n",
    "    filtered_dataframes = [df[df['filtered'] == True] for df in dataframes] # remove all data points below the baseline AKA removing noise\n",
    "    # Initialize lists for the cluster features\n",
    "    for df in filtered_dataframes:\n",
    "        # Extract the relevant columns\n",
    "        X = df[['phase_angle', 'peak_amplitude']].values\n",
    "        \n",
    "        # Apply DBSCAN to the data\n",
    "        dbscan = DBSCAN(eps=7, min_samples=8)\n",
    "        dbscan.fit(X)\n",
    "        labels = dbscan.labels_\n",
    "        \n",
    "        # Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "        df_copy = df.copy()\n",
    "        df_copy['cluster_length'] = np.nan\n",
    "        df_copy['cluster_height'] = np.nan\n",
    "        df_copy['cluster_gradient_tr'] = np.nan\n",
    "        df_copy['cluster_gradient_tl'] = np.nan\n",
    "        \n",
    "        # Add the cluster labels to the dataframe copy\n",
    "        df_copy['cluster'] = labels\n",
    "        \n",
    "        # Access cluster information\n",
    "        clusters = set(labels)\n",
    "        for cluster in clusters:\n",
    "            if cluster != -1:\n",
    "                # Get the points belonging to the cluster\n",
    "                cluster_points = X[labels == cluster]\n",
    "\n",
    "                \n",
    "                # Calculate cluster features\n",
    "                cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "                cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "                cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "                cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "                \n",
    "                # Add new columns for cluster features to the dataframe copy\n",
    "                df_copy.loc[labels == cluster, 'cluster_length'] = cluster_length\n",
    "                df_copy.loc[labels == cluster, 'cluster_height'] = cluster_height\n",
    "                df_copy.loc[labels == cluster, 'cluster_gradient_tr'] = cluster_gradient_tr\n",
    "                df_copy.loc[labels == cluster, 'cluster_gradient_tl'] = cluster_gradient_tl\n",
    "        \n",
    "        # Assign the cluster labels and features to the original dataframe\n",
    "        df.loc[:, 'cluster'] = df_copy['cluster']\n",
    "        df.loc[:, 'cluster_length'] = df_copy['cluster_length']\n",
    "        df.loc[:, 'cluster_height'] = df_copy['cluster_height']\n",
    "        df.loc[:, 'cluster_gradient_tr'] = df_copy['cluster_gradient_tr']\n",
    "        df.loc[:, 'cluster_gradient_tl'] = df_copy['cluster_gradient_tl']\n",
    "        \n",
    "        # # Plot the clustered data\n",
    "        # plt.scatter(df['phase_angle'], df['peak_amplitude'], c=df['cluster'], cmap='viridis', s=8)\n",
    "        # plt.title(df['filename'].iloc[0])\n",
    "        # plt.xlabel('Phase angle')\n",
    "        # plt.ylabel('Peak amplitude')\n",
    "        # plt.show()\n",
    "\n",
    "    # once the relevant graphs have been removed, convert list of dataframes into 1 big dataframe\n",
    "    big_df = pd.concat(filtered_dataframes, ignore_index=True)\n",
    "    big_df.fillna(0, inplace=True) # replace missing values with 0\n",
    "    # Calculate additional clustering features based on the cluster labels\n",
    "    # Calculate additional clustering features based on the cluster labels\n",
    "    clusters = []\n",
    "    for filename, file_df in big_df.groupby('filename'):\n",
    "        # Assume the cluster labels are stored in the 'cluster' column\n",
    "        labels = file_df['cluster'].values\n",
    "        unique_labels = np.unique(labels)\n",
    "        cluster_count = len(unique_labels)\n",
    "        if cluster_count > 0:\n",
    "            # Calculate additional clustering features based on the cluster labels\n",
    "            # For example, cluster_length, cluster_height, etc.\n",
    "            # Append the calculated features to the clusters list\n",
    "            cluster_lengths = []\n",
    "            cluster_heights = []\n",
    "            cluster_gradient_trs = []\n",
    "            cluster_gradient_tls = []\n",
    "            for i in range(4):\n",
    "                if i < len(unique_labels):\n",
    "                    label = unique_labels[i]\n",
    "                    cluster_points = file_df[file_df['cluster'] == label][['phase_angle', 'peak_amplitude']].values\n",
    "                    if len(cluster_points) > 1:\n",
    "                        cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "                        cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "                        cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "                        cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "                        cluster_lengths.append(cluster_length)\n",
    "                        cluster_heights.append(cluster_height)\n",
    "                        cluster_gradient_trs.append(cluster_gradient_tr)\n",
    "                        cluster_gradient_tls.append(cluster_gradient_tl)\n",
    "                else:\n",
    "                    # Fill in zeros for any missing clusters\n",
    "                    cluster_lengths.append(0)\n",
    "                    cluster_heights.append(0)\n",
    "                    cluster_gradient_trs.append(0)\n",
    "                    cluster_gradient_tls.append(0)\n",
    "            clusters.append({'filename': filename, 'cluster_count': cluster_count, 'cluster_lengths': cluster_lengths, 'cluster_heights': cluster_heights, 'cluster_gradient_trs': cluster_gradient_trs, 'cluster_gradient_tls': cluster_gradient_tls})\n",
    "\n",
    "    # Convert the clusters list to a dataframe\n",
    "    clusters_df = pd.DataFrame(clusters)\n",
    "\n",
    "    final_df = clusters_df\n",
    "\n",
    "    print(final_df)\n",
    "\n",
    "    df = final_df\n",
    "    # Expand the list columns into separate columns\n",
    "    expanded_df = pd.concat([df.drop(['cluster_lengths', 'cluster_heights', 'cluster_gradient_trs', 'cluster_gradient_tls'], axis=1),\n",
    "                            df['cluster_lengths'].apply(pd.Series).add_prefix('cluster_length_'),\n",
    "                            df['cluster_heights'].apply(pd.Series).add_prefix('cluster_height_'),\n",
    "                            df['cluster_gradient_trs'].apply(pd.Series).add_prefix('cluster_gradient_tr_'),\n",
    "                            df['cluster_gradient_tls'].apply(pd.Series).add_prefix('cluster_gradient_tl_')],\n",
    "                            axis=1)\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    expanded_df.fillna(0, inplace=True)\n",
    "    print(expanded_df)\n",
    "    return expanded_df\n",
    "\n",
    "def getFeatures(final_df):\n",
    "    features = final_df.columns.tolist()\n",
    "    features.remove('filename')\n",
    "    # features.remove('cluster_count')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to apply the trained model onto any new test dataset, the new test dataset must have the same number of features\n",
    "# meaning that we must apply the HDBScan clustering to the new test data, generate its columns, before finally \n",
    "# loading it into the model using predictions = model.predict(new_data[features])\n",
    "\n",
    "# step 1: load folder containing all testing files into a list of dataframes\n",
    "dirpath = \"test\" # name of folder\n",
    "to_be_predicted_df = load_dataframes(dirpath)\n",
    "\n",
    "# step 2: preprocess the data, which entails removing noise, getting cluster information for comparison, and dataframe conversion to include only the relevant features\n",
    "to_be_predicted_df = preprocess_data(to_be_predicted_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = getFeatures(to_be_predicted_df)\n",
    "X = to_be_predicted_df[f]\n",
    "predictions = DecisionTreeClassifierModel.predict(X)\n",
    "# print(getFeatures(to_be_predicted_df).dtypes)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_df = load_dataframes(dirpath)\n",
    "for i, dataframe in enumerate(placeholder_df):\n",
    "    plt.scatter(dataframe['phase_angle'], dataframe['peak_amplitude'], s=8)\n",
    "    plt.title(f\"{dataframe['filename'].iloc[0]} (Prediction: {predictions[i]})\")\n",
    "    plt.xlabel('Phase angle')\n",
    "    plt.ylabel('Peak amplitude')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
