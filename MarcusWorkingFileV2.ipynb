{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from scipy.io.wavfile import read\n",
    "import hdbscan\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "from datetime import datetime, timezone\n",
    "import datetime\n",
    "from scipy.stats import norm\n",
    "import padasip as pa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import OPTICS\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "import copy\n",
    "from scipy.stats import mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveform(filepath):\n",
    "    with open(filepath) as dataFile:\n",
    "        data = dataFile.read()\n",
    "        # Check for missing commas and insert them\n",
    "        pattern = r'(\\{[^{}]*\"[^\"]*\"[^{}]*\\})(?=[^{}]*\\{)'\n",
    "        data = re.sub(pattern, r'\\1,', data)\n",
    "\n",
    "        obj = data[data.find('{') : data.rfind('}')+1]\n",
    "        jsonObj = json.loads(obj)\n",
    "        \n",
    "        if jsonObj['data']:\n",
    "            waveform_data = pd.DataFrame(jsonObj['data'], columns=[\"peak_amplitude\",\"integral\",\"phase_angle\",\"cycle_number\",\"rise_time\",\"pulse_width\"])    \n",
    "            return waveform_data\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(dirpath):\n",
    "    dataframes = []\n",
    "    for filename in os.listdir(dirpath):\n",
    "        if filename.endswith(\".js\"):\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            waveform = load_waveform(filepath)\n",
    "            if not waveform.empty:\n",
    "                waveform['filename'] = filename # Add filename as a column\n",
    "                dataframes.append(waveform)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "dirpath1 = \"ptest\"\n",
    "dirpath2 = \"ntest\"\n",
    "dirpath3 = \"utest\"\n",
    "pdf = load_dataframes(dirpath1)\n",
    "ndf = load_dataframes(dirpath2)\n",
    "udf = load_dataframes(dirpath3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_vars = {} # create a dictionary with the assigned dependent variables mapped to the filename as the key\n",
    "for df, y in zip([pdf, ndf, udf], [1, 2, 0]): # 1 is positive, 2 is negative, 0 is unclassified\n",
    "    for filename in pd.concat(df)['filename'].unique():\n",
    "        dependent_vars[filename] = y\n",
    "# this is used to assign the dependent variables to the filename in the transformed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all list of dataframes into a single list of dataframes once dependent variables have been assigned\n",
    "dataframes = pdf + ndf + udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0              -0.24      1219          253          8824          0   \n",
      "1              -0.24      2616           34          8825          0   \n",
      "2               0.00     -2638          197          8825          0   \n",
      "3              -0.24       417          246          8825          0   \n",
      "4              -0.24      2149           33          8826          0   \n",
      "...              ...       ...          ...           ...        ...   \n",
      "5632            0.25       685          315          9322          0   \n",
      "5633            0.00       563           35          9323          0   \n",
      "5634            0.25     -1509          353          9323          0   \n",
      "5635            0.25      -539           22          9324          0   \n",
      "5636            0.25       684           93          9324          0   \n",
      "\n",
      "      pulse_width    filename  \n",
      "0               0  1047582.js  \n",
      "1               0  1047582.js  \n",
      "2               0  1047582.js  \n",
      "3               0  1047582.js  \n",
      "4               0  1047582.js  \n",
      "...           ...         ...  \n",
      "5632            0  1047582.js  \n",
      "5633            0  1047582.js  \n",
      "5634            0  1047582.js  \n",
      "5635            0  1047582.js  \n",
      "5636            0  1047582.js  \n",
      "\n",
      "[5637 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0                3.00      1381          314         14019         17   \n",
      "1                2.25      3227          315         14019         19   \n",
      "2                2.25      2651          316         14019         17   \n",
      "3                2.75      1601          317         14019          0   \n",
      "4                3.25      1615          318         14019         48   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "15501            1.50    -23367          338         14509          0   \n",
      "15502            2.50     -7968          338         14509          0   \n",
      "15503            2.25     -3693          338         14509        210   \n",
      "15504            2.25    -13520          338         14509          0   \n",
      "15505            2.00     -9817          339         14509          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0               20  1051807.js  \n",
      "1               19  1051807.js  \n",
      "2               17  1051807.js  \n",
      "3               20  1051807.js  \n",
      "4               52  1051807.js  \n",
      "...            ...         ...  \n",
      "15501          421  1051807.js  \n",
      "15502           69  1051807.js  \n",
      "15503          211  1051807.js  \n",
      "15504           85  1051807.js  \n",
      "15505            0  1051807.js  \n",
      "\n",
      "[15506 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0                7.25      2040          214          1412          0   \n",
      "1                6.75     -2831          224          1412          0   \n",
      "2                7.75      3181          349          1412          0   \n",
      "3                9.00      2953          349          1412          3   \n",
      "4               10.00      2895          358          1412          1   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "12210            8.00      2215          195          1912          0   \n",
      "12211            7.50      2639          198          1912          0   \n",
      "12212            6.25      2464          210          1912          0   \n",
      "12213            9.25      4814          211          1912          0   \n",
      "12214            6.75      1789          213          1912          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0                0  1054130.js  \n",
      "1                0  1054130.js  \n",
      "2                0  1054130.js  \n",
      "3                5  1054130.js  \n",
      "4                3  1054130.js  \n",
      "...            ...         ...  \n",
      "12210            0  1054130.js  \n",
      "12211            0  1054130.js  \n",
      "12212            0  1054130.js  \n",
      "12213            3  1054130.js  \n",
      "12214            0  1054130.js  \n",
      "\n",
      "[12215 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0                4.50     15755          314         23697          2   \n",
      "1                5.50      1777          315         23697          2   \n",
      "2                4.25      8114          315         23697          2   \n",
      "3                6.25     14884          315         23697          2   \n",
      "4                4.25      3188          316         23697         13   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "21909            5.50     14914          202         24196          2   \n",
      "21910            5.75      7369          203         24196          0   \n",
      "21911            6.25     12955          204         24196          3   \n",
      "21912            5.50    -11218          204         24196          0   \n",
      "21913            5.50      9894          204         24196          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0              450  1056759.js  \n",
      "1                4  1056759.js  \n",
      "2              451  1056759.js  \n",
      "3               15  1056759.js  \n",
      "4               13  1056759.js  \n",
      "...            ...         ...  \n",
      "21909            4  1056759.js  \n",
      "21910            0  1056759.js  \n",
      "21911            5  1056759.js  \n",
      "21912            5  1056759.js  \n",
      "21913            2  1056759.js  \n",
      "\n",
      "[21914 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0                0.25     -2812           94         31940          0   \n",
      "1                0.50     -1915           96         31940          0   \n",
      "2                0.50       762           99         31940          0   \n",
      "3                0.25     -2242           99         31940          0   \n",
      "4                0.25      2020           99         31940          0   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "15921            0.50     -5647           76         32437          0   \n",
      "15922            0.50      2166           79         32437          0   \n",
      "15923            1.00      1879           80         32437          0   \n",
      "15924            0.25      3947           83         32437          0   \n",
      "15925           -0.49     -5558           84         32437        411   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0              111  1136583.js  \n",
      "1                0  1136583.js  \n",
      "2                0  1136583.js  \n",
      "3                0  1136583.js  \n",
      "4                0  1136583.js  \n",
      "...            ...         ...  \n",
      "15921            0  1136583.js  \n",
      "15922            0  1136583.js  \n",
      "15923            0  1136583.js  \n",
      "15924          208  1136583.js  \n",
      "15925            0  1136583.js  \n",
      "\n",
      "[15926 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0                6.00      2657          348         23788          0   \n",
      "1                5.75      4528          349         23788          0   \n",
      "2                6.25    -12911          349         23788          0   \n",
      "3                5.75      3808          349         23788          0   \n",
      "4                5.50      3449          350         23788          0   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "18887            4.25      3101          276         24278          0   \n",
      "18888            4.25      4399          276         24278          0   \n",
      "18889            4.75     -7255          277         24278          0   \n",
      "18890            4.00     -6219          277         24278          0   \n",
      "18891            4.25     -9961          278         24278          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0                0  1161850.js  \n",
      "1               19  1161850.js  \n",
      "2               19  1161850.js  \n",
      "3                0  1161850.js  \n",
      "4                0  1161850.js  \n",
      "...            ...         ...  \n",
      "18887            0  1161850.js  \n",
      "18888            0  1161850.js  \n",
      "18889            0  1161850.js  \n",
      "18890            0  1161850.js  \n",
      "18891            0  1161850.js  \n",
      "\n",
      "[18892 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0                6.00      3475          117         37730          0   \n",
      "1                7.75    -12632          121         37730          0   \n",
      "2                6.25    -14036          121         37730          0   \n",
      "3                5.50     -4588          123         37730          0   \n",
      "4                6.25     -8501          125         37730          0   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "13746            3.50     -1674          278         38228          0   \n",
      "13747            4.50      1473          279         38228          0   \n",
      "13748            3.50      4140          280         38228          0   \n",
      "13749            4.25     -6666          281         38228          0   \n",
      "13750            4.00      3144          281         38228          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0                0  1167250.js  \n",
      "1                0  1167250.js  \n",
      "2                0  1167250.js  \n",
      "3                0  1167250.js  \n",
      "4                0  1167250.js  \n",
      "...            ...         ...  \n",
      "13746            0  1167250.js  \n",
      "13747            0  1167250.js  \n",
      "13748            0  1167250.js  \n",
      "13749            0  1167250.js  \n",
      "13750            0  1167250.js  \n",
      "\n",
      "[13751 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0                4.75      1429           78         15241          0   \n",
      "1                5.00      2160           79         15241          0   \n",
      "2                4.75     -1469           81         15241          0   \n",
      "3                4.75      1098           84         15241          0   \n",
      "4                4.75     -4396           84         15241          0   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "14697            1.25     -2841          333         15729          0   \n",
      "14698            1.00      1417          335         15729          0   \n",
      "14699            2.00      1326          335         15729          0   \n",
      "14700            2.00     -1495          337         15729          0   \n",
      "14701            1.25       664          338         15729          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0                0  1001263.js  \n",
      "1                0  1001263.js  \n",
      "2                0  1001263.js  \n",
      "3                0  1001263.js  \n",
      "4                0  1001263.js  \n",
      "...            ...         ...  \n",
      "14697            0  1001263.js  \n",
      "14698            0  1001263.js  \n",
      "14699            0  1001263.js  \n",
      "14700            0  1001263.js  \n",
      "14701            0  1001263.js  \n",
      "\n",
      "[14702 rows x 7 columns],       peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0               1.25     -1600          307         26446          0   \n",
      "1               1.25       969          307         26446          0   \n",
      "2               0.75      -796          338         26446          0   \n",
      "3               1.25       372          342         26446          0   \n",
      "4               0.75      1182          343         26446          0   \n",
      "...              ...       ...          ...           ...        ...   \n",
      "7603            0.75       421          181         26945          0   \n",
      "7604            1.50     -1225          261         26945          0   \n",
      "7605            1.75      1582          324         26945          0   \n",
      "7606            1.75       966          333         26945          0   \n",
      "7607            1.75      -654          334         26945          0   \n",
      "\n",
      "      pulse_width    filename  \n",
      "0               0  1001948.js  \n",
      "1               0  1001948.js  \n",
      "2               0  1001948.js  \n",
      "3               0  1001948.js  \n",
      "4               0  1001948.js  \n",
      "...           ...         ...  \n",
      "7603            0  1001948.js  \n",
      "7604            0  1001948.js  \n",
      "7605            0  1001948.js  \n",
      "7606            0  1001948.js  \n",
      "7607            0  1001948.js  \n",
      "\n",
      "[7608 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0                7.00     -1587          120         12836          0   \n",
      "1                6.50      1335          124         12836          0   \n",
      "2                6.50      4182          125         12836          0   \n",
      "3                6.00      9137          127         12836          0   \n",
      "4                7.25     -1743          203         12836          0   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "14394            3.25      2360          102         13311          0   \n",
      "14395            2.50      1800          103         13311          0   \n",
      "14396            2.50      2296          104         13311          0   \n",
      "14397            2.75      1094          104         13311          0   \n",
      "14398            3.25      2891          104         13311          1   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0                0  1007375.js  \n",
      "1                0  1007375.js  \n",
      "2                0  1007375.js  \n",
      "3               16  1007375.js  \n",
      "4                0  1007375.js  \n",
      "...            ...         ...  \n",
      "14394            0  1007375.js  \n",
      "14395            0  1007375.js  \n",
      "14396            0  1007375.js  \n",
      "14397            0  1007375.js  \n",
      "14398           80  1007375.js  \n",
      "\n",
      "[14399 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0               23.50     18998          130         25705          0   \n",
      "1               22.25     56717          132         25705          0   \n",
      "2               23.00      3915          132         25705        347   \n",
      "3               24.25     19811          133         25705          0   \n",
      "4               25.25      8514          134         25705          0   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "16161           23.25     42394          199         26197          0   \n",
      "16162           25.50     18551          202         26197          0   \n",
      "16163           23.75     11732          203         26197          0   \n",
      "16164           24.75     23589          203         26197          0   \n",
      "16165           24.75     38869          204         26197          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0                0  1011083.js  \n",
      "1                0  1011083.js  \n",
      "2                0  1011083.js  \n",
      "3                0  1011083.js  \n",
      "4                0  1011083.js  \n",
      "...            ...         ...  \n",
      "16161            0  1011083.js  \n",
      "16162            0  1011083.js  \n",
      "16163            0  1011083.js  \n",
      "16164            0  1011083.js  \n",
      "16165            0  1011083.js  \n",
      "\n",
      "[16166 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0               19.25     28278          184         32457          0   \n",
      "1               18.75    -51251          193         32457          0   \n",
      "2               18.50    -51765          203         32457          0   \n",
      "3               18.00    -59966          236         32457          0   \n",
      "4               19.25    -43550          239         32457          0   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "12192           17.25    -67905          319         32948          0   \n",
      "12193           19.25     32674          322         32948         35   \n",
      "12194           17.50     19992          322         32948          0   \n",
      "12195           17.00     22125          323         32948          0   \n",
      "12196           16.75   -100174          323         32948          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0                5  1015896.js  \n",
      "1                5  1015896.js  \n",
      "2                5  1015896.js  \n",
      "3                5  1015896.js  \n",
      "4                5  1015896.js  \n",
      "...            ...         ...  \n",
      "12192           37  1015896.js  \n",
      "12193           37  1015896.js  \n",
      "12194           37  1015896.js  \n",
      "12195           37  1015896.js  \n",
      "12196           37  1015896.js  \n",
      "\n",
      "[12197 rows x 7 columns],       peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0               0.00      1478          227         13154          0   \n",
      "1               0.25       789          242         13154          0   \n",
      "2               0.50      1586          248         13154          0   \n",
      "3               1.75       484          248         13154          0   \n",
      "4               0.00     -3177          251         13154          0   \n",
      "...              ...       ...          ...           ...        ...   \n",
      "7647            0.50     -1593          240         13653          0   \n",
      "7648            0.00      1372          242         13653          0   \n",
      "7649           -0.49      2232          242         13653        312   \n",
      "7650            0.25      2331          253         13653          0   \n",
      "7651            0.00      1449          255         13653          0   \n",
      "\n",
      "      pulse_width    filename  \n",
      "0               0  1017547.js  \n",
      "1               0  1017547.js  \n",
      "2               0  1017547.js  \n",
      "3               0  1017547.js  \n",
      "4               0  1017547.js  \n",
      "...           ...         ...  \n",
      "7647            0  1017547.js  \n",
      "7648            0  1017547.js  \n",
      "7649            0  1017547.js  \n",
      "7650            0  1017547.js  \n",
      "7651            0  1017547.js  \n",
      "\n",
      "[7652 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0               14.00      8452          259         29944          2   \n",
      "1               13.25     -5594          259         29944        155   \n",
      "2               13.00     37223          259         29944         10   \n",
      "3               14.00      3878          260         29944          2   \n",
      "4               13.00     -8968          260         29944          0   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "18261           25.00    -12039          356         30432          0   \n",
      "18262           26.00      8316          356         30432          0   \n",
      "18263           24.25     19699          356         30432          0   \n",
      "18264           23.75     49098          358         30432          0   \n",
      "18265           23.00     45848          359         30432          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0               13  1029777.js  \n",
      "1              156  1029777.js  \n",
      "2              363  1029777.js  \n",
      "3                2  1029777.js  \n",
      "4                0  1029777.js  \n",
      "...            ...         ...  \n",
      "18261            3  1029777.js  \n",
      "18262            3  1029777.js  \n",
      "18263            3  1029777.js  \n",
      "18264            3  1029777.js  \n",
      "18265            3  1029777.js  \n",
      "\n",
      "[18266 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0                7.00      3336           33         30862          2   \n",
      "1                7.50     10040           33         30862          4   \n",
      "2                6.75    -10229           33         30862          0   \n",
      "3                7.00      9783           34         30862          1   \n",
      "4                7.25      9788           34         30862          2   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "18883           26.25     29122          172         31360          0   \n",
      "18884           25.50     -8328          174         31360        359   \n",
      "18885           24.25     11092          174         31360          0   \n",
      "18886           25.75      7891          175         31360          0   \n",
      "18887           25.25     46400          176         31360          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0               15  1089139.js  \n",
      "1                6  1089139.js  \n",
      "2                6  1089139.js  \n",
      "3                3  1089139.js  \n",
      "4                6  1089139.js  \n",
      "...            ...         ...  \n",
      "18883            0  1089139.js  \n",
      "18884            0  1089139.js  \n",
      "18885            0  1089139.js  \n",
      "18886            0  1089139.js  \n",
      "18887            0  1089139.js  \n",
      "\n",
      "[18888 rows x 7 columns],        peak_amplitude  integral  phase_angle  cycle_number  rise_time  \\\n",
      "0                0.75      -555           61         32931          0   \n",
      "1                1.25     -1142          109         32931          0   \n",
      "2                0.75       831          137         32931          0   \n",
      "3                0.75     -1592          140         32931          0   \n",
      "4                1.00       527          183         32931          0   \n",
      "...               ...       ...          ...           ...        ...   \n",
      "19810           11.00      4598          142         33402          0   \n",
      "19811           10.75      3785          143         33402          0   \n",
      "19812           10.75      4190          143         33402          0   \n",
      "19813           11.75      3388          143         33402          0   \n",
      "19814           11.00      4786          144         33402          0   \n",
      "\n",
      "       pulse_width    filename  \n",
      "0                0  1171468.js  \n",
      "1                0  1171468.js  \n",
      "2                0  1171468.js  \n",
      "3                0  1171468.js  \n",
      "4                0  1171468.js  \n",
      "...            ...         ...  \n",
      "19810            0  1171468.js  \n",
      "19811            0  1171468.js  \n",
      "19812            0  1171468.js  \n",
      "19813            1  1171468.js  \n",
      "19814            1  1171468.js  \n",
      "\n",
      "[19815 rows x 7 columns]]\n"
     ]
    }
   ],
   "source": [
    "print(dataframes) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataframe in dataframes: # show data into plots\n",
    "#     plt.scatter(dataframe['phase_angle'], dataframe['peak_amplitude'], s=8)\n",
    "#     plt.title(dataframe['filename'].iloc[0])\n",
    "#     plt.xlabel('Phase angle')\n",
    "#     plt.ylabel('Peak amplitude')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_baseline(bucket_data, kmeans, predicted_clusters, threshold=5): # algorithm to determine baselines\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    centroid_distance = abs(centroids[1] - centroids[0])\n",
    "\n",
    "    if centroid_distance > threshold:\n",
    "        # Clusters far apart - Outcome 1\n",
    "        lower_centroid_index = np.argmin(centroids)\n",
    "        lower_cluster = bucket_data[predicted_clusters == lower_centroid_index]\n",
    "        return max(lower_cluster['peak_amplitude'])\n",
    "    else:\n",
    "        # Clusters close together - Outcome 2\n",
    "        higher_centroid_index = np.argmax(centroids)\n",
    "        return centroids[higher_centroid_index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_dataframes(dataframes): # function to calculate base-lines and plot the graphs with the new added information\n",
    "    warnings.filterwarnings('ignore')\n",
    "    for df in dataframes:\n",
    "        # if df['peak_amplitude'].max() > 8: # filter out the negative\n",
    "        #     continue\n",
    "\n",
    "        # Bucket phase angles\n",
    "        df['bucket'] = pd.cut(df['phase_angle'], bins=range(0, 361, 10), labels=False)\n",
    "    \n",
    "        # Perform k-means clustering on each bucket and determine baselines\n",
    "        kmeans_results = []\n",
    "        bucket_baselines = []\n",
    "        for i in range(36):\n",
    "            bucket_data = df[df['bucket'] == i]\n",
    "            if not bucket_data.empty and len(bucket_data) > 1:\n",
    "                kmeans = KMeans(n_clusters=2, random_state=0, n_init=10)\n",
    "                kmeans.fit(bucket_data['peak_amplitude'].values.reshape(-1, 1))\n",
    "                kmeans_results.append((i, kmeans))\n",
    "\n",
    "                # Predict the clusters for bucket_data\n",
    "                predicted_clusters = kmeans.predict(bucket_data['peak_amplitude'].values.reshape(-1, 1))\n",
    "\n",
    "                # Pass the predicted clusters to the determine_baseline function\n",
    "                baseline = determine_baseline(bucket_data, kmeans, predicted_clusters)\n",
    "                bucket_baselines.append((i, baseline))\n",
    "            else:\n",
    "                kmeans_results.append((i, None))\n",
    "                bucket_baselines.append((i, None))\n",
    "\n",
    "        # Assign cluster labels to each data point\n",
    "        df['cluster'] = np.nan\n",
    "        for i, kmeans in kmeans_results:\n",
    "            if kmeans is not None:\n",
    "                mask = df['bucket'] == i\n",
    "                df.loc[mask, 'cluster'] = kmeans.predict(df.loc[mask, 'peak_amplitude'].values.reshape(-1, 1))\n",
    "\n",
    "        baselines = [baseline for _, baseline in bucket_baselines if baseline is not None]\n",
    "        basemode = mode(baselines).mode[0]\n",
    "\n",
    "        # Remove points below the baseline mode (inserted)\n",
    "        df.loc[:, 'filtered'] = df['peak_amplitude'] >= basemode\n",
    "\n",
    "        # # Scatter plot with bucketed phase angles and clustered data points\n",
    "        # plt.scatter(df['phase_angle'], df['peak_amplitude'], c=df['cluster'], cmap='viridis', s=8)\n",
    "        # plt.title(df['filename'].iloc[0])\n",
    "        # plt.xlabel('Phase angle')\n",
    "        # plt.ylabel('Peak amplitude')\n",
    "\n",
    "        # # Plot baselines\n",
    "        # for i, baseline in bucket_baselines:\n",
    "        #     if baseline is not None:\n",
    "        #         plt.hlines(baseline, i * 10, (i + 1) * 10 - 1, colors='r', linestyles='dashed')\n",
    "        # plt.hlines(basemode, 0, 359, colors='b', linestyles='solid')\n",
    "        # plt.show()\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dave\\Documents\\GitHub\\ML-Bangbang\\MarcusWorkingFileV2.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Dave/Documents/GitHub/ML-Bangbang/MarcusWorkingFileV2.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataframes \u001b[39m=\u001b[39m denoise_dataframes(dataframes)\n",
      "\u001b[1;32mc:\\Users\\Dave\\Documents\\GitHub\\ML-Bangbang\\MarcusWorkingFileV2.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dave/Documents/GitHub/ML-Bangbang/MarcusWorkingFileV2.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m bucket_data\u001b[39m.\u001b[39mempty \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(bucket_data) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dave/Documents/GitHub/ML-Bangbang/MarcusWorkingFileV2.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, n_init\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Dave/Documents/GitHub/ML-Bangbang/MarcusWorkingFileV2.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     kmeans\u001b[39m.\u001b[39;49mfit(bucket_data[\u001b[39m'\u001b[39;49m\u001b[39mpeak_amplitude\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dave/Documents/GitHub/ML-Bangbang/MarcusWorkingFileV2.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     kmeans_results\u001b[39m.\u001b[39mappend((i, kmeans))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dave/Documents/GitHub/ML-Bangbang/MarcusWorkingFileV2.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# Predict the clusters for bucket_data\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1417\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitialization complete\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1416\u001b[0m \u001b[39m# run a k-means once\u001b[39;00m\n\u001b[1;32m-> 1417\u001b[0m labels, inertia, centers, n_iter_ \u001b[39m=\u001b[39m kmeans_single(\n\u001b[0;32m   1418\u001b[0m     X,\n\u001b[0;32m   1419\u001b[0m     sample_weight,\n\u001b[0;32m   1420\u001b[0m     centers_init,\n\u001b[0;32m   1421\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m   1422\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m   1423\u001b[0m     tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tol,\n\u001b[0;32m   1424\u001b[0m     x_squared_norms\u001b[39m=\u001b[39;49mx_squared_norms,\n\u001b[0;32m   1425\u001b[0m     n_threads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_threads,\n\u001b[0;32m   1426\u001b[0m )\n\u001b[0;32m   1428\u001b[0m \u001b[39m# determine if these results are the best so far\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m \u001b[39m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m \u001b[39m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \u001b[39m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m \u001b[39m# permuted labels, due to rounding errors)\u001b[39;00m\n\u001b[0;32m   1433\u001b[0m \u001b[39mif\u001b[39;00m best_inertia \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   1434\u001b[0m     inertia \u001b[39m<\u001b[39m best_inertia\n\u001b[0;32m   1435\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_same_clustering(labels, best_labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_clusters)\n\u001b[0;32m   1436\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:646\u001b[0m, in \u001b[0;36m_kmeans_single_lloyd\u001b[1;34m(X, sample_weight, centers_init, max_iter, verbose, x_squared_norms, tol, n_threads)\u001b[0m\n\u001b[0;32m    642\u001b[0m strict_convergence \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[39m# Threadpoolctl context to limit the number of threads in second level of\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[39m# nested parallelism (i.e. BLAS) to avoid oversubscription.\u001b[39;00m\n\u001b[1;32m--> 646\u001b[0m \u001b[39mwith\u001b[39;00m threadpool_limits(limits\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, user_api\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mblas\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[0;32m    647\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iter):\n\u001b[0;32m    648\u001b[0m         lloyd_iter(\n\u001b[0;32m    649\u001b[0m             X,\n\u001b[0;32m    650\u001b[0m             sample_weight,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    657\u001b[0m             n_threads,\n\u001b[0;32m    658\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\fixes.py:149\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[1;34m(limits, user_api)\u001b[0m\n\u001b[0;32m    147\u001b[0m controller \u001b[39m=\u001b[39m _get_threadpool_controller()\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m controller \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[39mreturn\u001b[39;00m controller\u001b[39m.\u001b[39;49mlimit(limits\u001b[39m=\u001b[39;49mlimits, user_api\u001b[39m=\u001b[39;49muser_api)\n\u001b[0;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[39mreturn\u001b[39;00m threadpoolctl\u001b[39m.\u001b[39mthreadpool_limits(limits\u001b[39m=\u001b[39mlimits, user_api\u001b[39m=\u001b[39muser_api)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\threadpoolctl.py:506\u001b[0m, in \u001b[0;36mThreadpoolController.limit\u001b[1;34m(self, limits, user_api)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39m@_format_docstring\u001b[39m(\n\u001b[0;32m    456\u001b[0m     USER_APIS\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(api) \u001b[39mfor\u001b[39;00m api \u001b[39min\u001b[39;00m _ALL_USER_APIS),\n\u001b[0;32m    457\u001b[0m     BLAS_LIBS\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(_ALL_BLAS_LIBRARIES),\n\u001b[0;32m    458\u001b[0m     OPENMP_LIBS\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(_ALL_OPENMP_LIBRARIES),\n\u001b[0;32m    459\u001b[0m )\n\u001b[0;32m    460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlimit\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m, limits\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, user_api\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    461\u001b[0m     \u001b[39m\"\"\"Change the maximal number of threads that can be used in thread pools.\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \n\u001b[0;32m    463\u001b[0m \u001b[39m    This function returns an object that can be used either as a callable (the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39m        - If None, this function will apply to all supported libraries.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m _ThreadpoolLimiter(\u001b[39mself\u001b[39;49m, limits\u001b[39m=\u001b[39;49mlimits, user_api\u001b[39m=\u001b[39;49muser_api)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\threadpoolctl.py:166\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__init__\u001b[1;34m(self, controller, limits, user_api)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_limits, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_user_api, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prefixes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params(\n\u001b[0;32m    163\u001b[0m     limits, user_api\n\u001b[0;32m    164\u001b[0m )\n\u001b[0;32m    165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_controller\u001b[39m.\u001b[39minfo()\n\u001b[1;32m--> 166\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_threadpool_limits()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\threadpoolctl.py:299\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter._set_threadpool_limits\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m num_threads \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m     lib_controller\u001b[39m.\u001b[39;49mset_num_threads(num_threads)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\threadpoolctl.py:865\u001b[0m, in \u001b[0;36mOpenBLASController.set_num_threads\u001b[1;34m(self, num_threads)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_num_threads\u001b[39m(\u001b[39mself\u001b[39m, num_threads):\n\u001b[0;32m    857\u001b[0m     set_func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\n\u001b[0;32m    858\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dynlib,\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mopenblas_set_num_threads\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    863\u001b[0m         ),\n\u001b[0;32m    864\u001b[0m     )\n\u001b[1;32m--> 865\u001b[0m     \u001b[39mreturn\u001b[39;00m set_func(num_threads)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataframes = denoise_dataframes(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframes[2]) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframes = [df[df['filtered'] == True] for df in dataframes] # remove all data points below the baseline AKA removing noise\n",
    "print(filtered_dataframes[1]) # check if remaining values are True, meaning that the data points remaining are the ones that we want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in filtered_dataframes: # show new graphs with threshold removed\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(dataframe['phase_angle'], dataframe['peak_amplitude'], s=8)\n",
    "    ax.set_title(dataframe['filename'].iloc[0])\n",
    "    ax.set_xlabel('Phase angle')\n",
    "    ax.set_ylabel('Peak amplitude')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for the cluster features\n",
    "for df in filtered_dataframes:\n",
    "    # Extract the relevant columns\n",
    "    X = df[['phase_angle', 'peak_amplitude']].values\n",
    "    \n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=5,\n",
    "    min_samples=2,\n",
    "    cluster_selection_epsilon=3,\n",
    "    cluster_selection_method='eom'  # or 'leaf'\n",
    "    )\n",
    "    clusterer.fit(X)\n",
    "    labels = clusterer.labels_\n",
    "\n",
    "    # Count the number of points in each cluster\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    cluster_counts = dict(zip(unique, counts))\n",
    "\n",
    "    # Sort clusters by size and keep only the four largest clusters\n",
    "    sorted_clusters = sorted(cluster_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    if sorted_clusters[0][0] == -1:  # Remove the noise cluster (-1) if it's present\n",
    "        sorted_clusters = sorted_clusters[1:5]\n",
    "\n",
    "    # Create a mask for the data points belonging to the largest clusters\n",
    "    mask = np.isin(labels, [c[0] for c in sorted_clusters])\n",
    "\n",
    "    # Apply the mask to the labels\n",
    "    filtered_labels = np.where(mask, labels, -1)\n",
    "\n",
    "    # Replace the original labels with the filtered_labels\n",
    "    df['cluster'] = filtered_labels\n",
    "    \n",
    "    # Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "    df_copy = df.copy()\n",
    "    df_copy['cluster_length'] = np.nan\n",
    "    df_copy['cluster_height'] = np.nan\n",
    "    df_copy['cluster_gradient_tr'] = np.nan\n",
    "    df_copy['cluster_gradient_tl'] = np.nan\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Access cluster information\n",
    "    clusters = set(labels)\n",
    "    for cluster in clusters:\n",
    "        if cluster != -1:\n",
    "            # Get the points belonging to the cluster\n",
    "            cluster_points = X[labels == cluster]\n",
    "\n",
    "            \n",
    "            # Calculate cluster features\n",
    "            cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "            cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "            cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "            cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "            \n",
    "            # Add new columns for cluster features to the dataframe copy\n",
    "            df_copy.loc[labels == cluster, 'cluster_length'] = cluster_length\n",
    "            df_copy.loc[labels == cluster, 'cluster_height'] = cluster_height\n",
    "            df_copy.loc[labels == cluster, 'cluster_gradient_tr'] = cluster_gradient_tr\n",
    "            df_copy.loc[labels == cluster, 'cluster_gradient_tl'] = cluster_gradient_tl\n",
    "    \n",
    "    # Assign the cluster labels and features to the original dataframe\n",
    "    df.loc[:, 'cluster'] = df_copy['cluster']\n",
    "    df.loc[:, 'cluster_length'] = df_copy['cluster_length']\n",
    "    df.loc[:, 'cluster_height'] = df_copy['cluster_height']\n",
    "    df.loc[:, 'cluster_gradient_tr'] = df_copy['cluster_gradient_tr']\n",
    "    df.loc[:, 'cluster_gradient_tl'] = df_copy['cluster_gradient_tl']\n",
    "    \n",
    "    # Plot the clustered data\n",
    "    plt.scatter(df['phase_angle'], df['peak_amplitude'], c=df['cluster'], cmap='viridis', s=8)\n",
    "    plt.title(df['filename'].iloc[0])\n",
    "    plt.xlabel('Phase angle')\n",
    "    plt.ylabel('Peak amplitude')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_cluster_bounds(cluster_points):\n",
    "#     x_min, x_max = cluster_points[:, 0].min(), cluster_points[:, 0].max()\n",
    "#     y_min, y_max = cluster_points[:, 1].min(), cluster_points[:, 1].max()\n",
    "#     plt.plot([x_min, x_max], [y_min, y_min], 'k-', linewidth=2)\n",
    "#     plt.plot([x_max, x_max], [y_min, y_max], 'k-', linewidth=2)\n",
    "#     plt.plot([x_max, x_min], [y_max, y_max], 'k-', linewidth=2)\n",
    "#     plt.plot([x_min, x_min], [y_max, y_min], 'k-', linewidth=2)\n",
    "\n",
    "\n",
    "# for df in filtered_dataframes:\n",
    "#     # Extract the relevant columns\n",
    "#     X = df[['phase_angle', 'peak_amplitude']].values\n",
    "    \n",
    "#     # Apply DBSCAN to the data\n",
    "#     dbscan = DBSCAN(eps=10, min_samples=10)\n",
    "#     dbscan.fit(X)\n",
    "#     labels = dbscan.labels_\n",
    "    \n",
    "#     # Add the cluster labels to the dataframe\n",
    "#     df['cluster'] = labels\n",
    "    \n",
    "#     # Create new columns for each cluster label\n",
    "#     dummies = pd.get_dummies(df['cluster'], prefix='cluster')\n",
    "#     df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "#     # Access cluster information\n",
    "#     clusters = set(labels)\n",
    "#     for cluster in clusters:\n",
    "#         if cluster != -1:\n",
    "#             # Get the points belonging to the cluster\n",
    "#             cluster_points = X[labels == cluster]\n",
    "            \n",
    "#             # Calculate cluster features\n",
    "#             cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "#             cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "#             cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "#             cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "            \n",
    "#             # Print cluster information\n",
    "#             print(f\"Cluster {cluster}: Length={cluster_length}, Height={cluster_height}, GradientTR={cluster_gradient_tr}, GradientTL={cluster_gradient_tl}\")\n",
    "            \n",
    "#             # Plot the boundaries of the cluster\n",
    "#             plot_cluster_bounds(cluster_points)\n",
    "    \n",
    "#     # Plot the clustered data\n",
    "#     plt.scatter(df['phase_angle'], df['peak_amplitude'], c=df['cluster'], cmap='viridis', s=8)\n",
    "#     plt.title(df['filename'].iloc[0])\n",
    "#     plt.xlabel('Phase angle')\n",
    "#     plt.ylabel('Peak amplitude')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_dataframes[0]) # integers in the cluster column indicate the cluster that they belong to, if its a -1 it means that it is an outlier and is not included in any clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once the relevant graphs have been removed, convert list of dataframes into 1 big dataframe\n",
    "big_df = pd.concat(filtered_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.isna().sum() # check number of empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.fillna(0, inplace=True) # replace missing values with 0\n",
    "big_df.isna().sum() # check again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate additional clustering features based on the cluster labels\n",
    "# clusters = []\n",
    "# for filename, file_df in big_df.groupby('filename'):\n",
    "#     # Assume the cluster labels are stored in the 'cluster' column\n",
    "#     labels = file_df['cluster'].values\n",
    "#     unique_labels = np.unique(labels)\n",
    "#     cluster_count = len(unique_labels)\n",
    "#     if cluster_count > 0:\n",
    "#         # Calculate additional clustering features based on the cluster labels\n",
    "#         # For example, cluster_length, cluster_height, etc.\n",
    "#         # Append the calculated features to the clusters list\n",
    "#         cluster_lengths = []\n",
    "#         cluster_heights = []\n",
    "#         cluster_gradient_trs = []\n",
    "#         cluster_gradient_tls = []\n",
    "#         for label in unique_labels:\n",
    "#             cluster_points = file_df[file_df['cluster'] == label][['phase_angle', 'peak_amplitude']].values\n",
    "#             if len(cluster_points) > 1:\n",
    "#                 cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "#                 cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "#                 cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "#                 cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "#                 cluster_lengths.append(cluster_length)\n",
    "#                 cluster_heights.append(cluster_height)\n",
    "#                 cluster_gradient_trs.append(cluster_gradient_tr)\n",
    "#                 cluster_gradient_tls.append(cluster_gradient_tl)\n",
    "#         clusters.append({'filename': filename, 'cluster_count': cluster_count, 'cluster_lengths': cluster_lengths, 'cluster_heights': cluster_heights, 'cluster_gradient_trs': cluster_gradient_trs, 'cluster_gradient_tls': cluster_gradient_tls})\n",
    "\n",
    "# # Convert the clusters list to a dataframe\n",
    "# clusters_df = pd.DataFrame(clusters)\n",
    "\n",
    "# final_df = clusters_df\n",
    "\n",
    "# final_df['anomaly'] = final_df['filename'].apply(lambda x: dependent_vars.get(x, 0)) # call dictionary with filename keys mapped to dependent variables respective to each file\n",
    "\n",
    "# print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional clustering features based on the cluster labels\n",
    "clusters = []\n",
    "for filename, file_df in big_df.groupby('filename'):\n",
    "    # Assume the cluster labels are stored in the 'cluster' column\n",
    "    labels = file_df['cluster'].values\n",
    "    unique_labels = np.unique(labels)\n",
    "    cluster_count = len(unique_labels)\n",
    "    if cluster_count > 0:\n",
    "        # Calculate additional clustering features based on the cluster labels\n",
    "        # For example, cluster_length, cluster_height, etc.\n",
    "        # Append the calculated features to the clusters list\n",
    "        cluster_lengths = []\n",
    "        cluster_heights = []\n",
    "        cluster_gradient_trs = []\n",
    "        cluster_gradient_tls = []\n",
    "        for i in range(4):\n",
    "            if i < len(unique_labels):\n",
    "                label = unique_labels[i]\n",
    "                cluster_points = file_df[file_df['cluster'] == label][['phase_angle', 'peak_amplitude']].values\n",
    "                if len(cluster_points) > 1:\n",
    "                    cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "                    cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "                    cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "                    cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "                    cluster_lengths.append(cluster_length)\n",
    "                    cluster_heights.append(cluster_height)\n",
    "                    cluster_gradient_trs.append(cluster_gradient_tr)\n",
    "                    cluster_gradient_tls.append(cluster_gradient_tl)\n",
    "            else:\n",
    "                # Fill in zeros for any missing clusters\n",
    "                cluster_lengths.append(0)\n",
    "                cluster_heights.append(0)\n",
    "                cluster_gradient_trs.append(0)\n",
    "                cluster_gradient_tls.append(0)\n",
    "        clusters.append({'filename': filename, 'cluster_count': cluster_count, 'cluster_lengths': cluster_lengths, 'cluster_heights': cluster_heights, 'cluster_gradient_trs': cluster_gradient_trs, 'cluster_gradient_tls': cluster_gradient_tls})\n",
    "\n",
    "# Convert the clusters list to a dataframe\n",
    "clusters_df = pd.DataFrame(clusters)\n",
    "\n",
    "final_df = clusters_df\n",
    "\n",
    "final_df['anomaly'] = final_df['filename'].apply(lambda x: dependent_vars.get(x, 0)) # call dictionary with filename keys mapped to dependent variables respective to each file\n",
    "\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('testforpresentation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df\n",
    "# Expand the list columns into separate columns\n",
    "expanded_df = pd.concat([df.drop(['cluster_lengths', 'cluster_heights', 'cluster_gradient_trs', 'cluster_gradient_tls'], axis=1),\n",
    "                         df['cluster_lengths'].apply(pd.Series).add_prefix('cluster_length_'),\n",
    "                         df['cluster_heights'].apply(pd.Series).add_prefix('cluster_height_'),\n",
    "                         df['cluster_gradient_trs'].apply(pd.Series).add_prefix('cluster_gradient_tr_'),\n",
    "                         df['cluster_gradient_tls'].apply(pd.Series).add_prefix('cluster_gradient_tl_')],\n",
    "                        axis=1)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "expanded_df.fillna(0, inplace=True)\n",
    "\n",
    "# be sure to include this in your model training code so that the filename and anomalies are not calculated as features!\n",
    "# Define the features and target variables:\n",
    "# features = expanded_df.columns.tolist()\n",
    "# features.remove('filename')\n",
    "# features.remove('anomaly')\n",
    "# target = 'anomaly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "expanded_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_dfs, test_dfs = train_test_split(expanded_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the features and target variables\n",
    "features = expanded_df.columns.tolist()\n",
    "features.remove('filename')\n",
    "features.remove('anomaly')\n",
    "target = 'anomaly'\n",
    "\n",
    "# Create a pipeline with a SimpleImputer and DecisionTreeClassifier\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "DecisionTreeClassifierModel = Pipeline([('imputer', imputer), ('classifier', classifier)])\n",
    "\n",
    "# Train the model\n",
    "DecisionTreeClassifierModel.fit(train_dfs[features], train_dfs[target])\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = DecisionTreeClassifierModel.predict(test_dfs[features])\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(test_dfs[target], predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_dfs[target], predictions)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with a SimpleImputer and LogisticRegression\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "LogisticRegressionModel = Pipeline([('imputer', imputer), ('classifier', classifier)])\n",
    "\n",
    "# Train the model\n",
    "LogisticRegressionModel.fit(train_dfs[features], train_dfs[target])\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions2 = LogisticRegressionModel.predict(test_dfs[features])\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(test_dfs[target], predictions2)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_dfs[target], predictions2)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with a SimpleImputer and RandomForestClassifier\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "RandomForestClassifierModel = Pipeline([('imputer', imputer), ('classifier', classifier)])\n",
    "\n",
    "# Train the model\n",
    "RandomForestClassifierModel.fit(train_dfs[features], train_dfs[target])\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions3 = RandomForestClassifierModel.predict(test_dfs[features])\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(test_dfs[target], predictions3)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_dfs[target], predictions3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_df)\n",
    "expanded_df.to_csv('test123.1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataframes):\n",
    "    dataframes = denoise_dataframes(dataframes) # de-noise the list of dataframes\n",
    "    filtered_dataframes = [df[df['filtered'] == True] for df in dataframes] # remove all data points below the baseline AKA removing noise\n",
    "    # Initialize lists for the cluster features\n",
    "    for df in filtered_dataframes:\n",
    "        # Extract the relevant columns\n",
    "        X = df[['phase_angle', 'peak_amplitude']].values\n",
    "        \n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=5,\n",
    "        min_samples=2,\n",
    "        cluster_selection_epsilon=3,\n",
    "        cluster_selection_method='eom'  # or 'leaf'\n",
    "        )\n",
    "        clusterer.fit(X)\n",
    "        labels = clusterer.labels_\n",
    "\n",
    "        # Count the number of points in each cluster\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        cluster_counts = dict(zip(unique, counts))\n",
    "\n",
    "        # Sort clusters by size and keep only the four largest clusters\n",
    "        sorted_clusters = sorted(cluster_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        if sorted_clusters[0][0] == -1:  # Remove the noise cluster (-1) if it's present\n",
    "            sorted_clusters = sorted_clusters[1:5]\n",
    "\n",
    "        # Create a mask for the data points belonging to the largest clusters\n",
    "        mask = np.isin(labels, [c[0] for c in sorted_clusters])\n",
    "\n",
    "        # Apply the mask to the labels\n",
    "        filtered_labels = np.where(mask, labels, -1)\n",
    "\n",
    "        # Replace the original labels with the filtered_labels\n",
    "        df['cluster'] = filtered_labels\n",
    "        \n",
    "        # Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "        df_copy = df.copy()\n",
    "        df_copy['cluster_length'] = np.nan\n",
    "        df_copy['cluster_height'] = np.nan\n",
    "        df_copy['cluster_gradient_tr'] = np.nan\n",
    "        df_copy['cluster_gradient_tl'] = np.nan\n",
    "        \n",
    "      \n",
    "        \n",
    "        # Access cluster information\n",
    "        clusters = set(labels)\n",
    "        for cluster in clusters:\n",
    "            if cluster != -1:\n",
    "                # Get the points belonging to the cluster\n",
    "                cluster_points = X[labels == cluster]\n",
    "\n",
    "                \n",
    "                # Calculate cluster features\n",
    "                cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "                cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "                cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "                cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "                \n",
    "                # Add new columns for cluster features to the dataframe copy\n",
    "                df_copy.loc[labels == cluster, 'cluster_length'] = cluster_length\n",
    "                df_copy.loc[labels == cluster, 'cluster_height'] = cluster_height\n",
    "                df_copy.loc[labels == cluster, 'cluster_gradient_tr'] = cluster_gradient_tr\n",
    "                df_copy.loc[labels == cluster, 'cluster_gradient_tl'] = cluster_gradient_tl\n",
    "        \n",
    "        # Assign the cluster labels and features to the original dataframe\n",
    "        df.loc[:, 'cluster'] = df_copy['cluster']\n",
    "        df.loc[:, 'cluster_length'] = df_copy['cluster_length']\n",
    "        df.loc[:, 'cluster_height'] = df_copy['cluster_height']\n",
    "        df.loc[:, 'cluster_gradient_tr'] = df_copy['cluster_gradient_tr']\n",
    "        df.loc[:, 'cluster_gradient_tl'] = df_copy['cluster_gradient_tl']\n",
    "        \n",
    "        # # Plot the clustered data\n",
    "        # plt.scatter(df['phase_angle'], df['peak_amplitude'], c=df['cluster'], cmap='viridis', s=8)\n",
    "        # plt.title(df['filename'].iloc[0])\n",
    "        # plt.xlabel('Phase angle')\n",
    "        # plt.ylabel('Peak amplitude')\n",
    "        # plt.show()\n",
    "\n",
    "    # once the relevant graphs have been removed, convert list of dataframes into 1 big dataframe\n",
    "    big_df = pd.concat(filtered_dataframes, ignore_index=True)\n",
    "    big_df.fillna(0, inplace=True) # replace missing values with 0\n",
    "    # Calculate additional clustering features based on the cluster labels\n",
    "    # Calculate additional clustering features based on the cluster labels\n",
    "    clusters = []\n",
    "    for filename, file_df in big_df.groupby('filename'):\n",
    "        # Assume the cluster labels are stored in the 'cluster' column\n",
    "        labels = file_df['cluster'].values\n",
    "        unique_labels = np.unique(labels)\n",
    "        cluster_count = len(unique_labels)\n",
    "        if cluster_count > 0:\n",
    "            # Calculate additional clustering features based on the cluster labels\n",
    "            # For example, cluster_length, cluster_height, etc.\n",
    "            # Append the calculated features to the clusters list\n",
    "            cluster_lengths = []\n",
    "            cluster_heights = []\n",
    "            cluster_gradient_trs = []\n",
    "            cluster_gradient_tls = []\n",
    "            for i in range(4):\n",
    "                if i < len(unique_labels):\n",
    "                    label = unique_labels[i]\n",
    "                    cluster_points = file_df[file_df['cluster'] == label][['phase_angle', 'peak_amplitude']].values\n",
    "                    if len(cluster_points) > 1:\n",
    "                        cluster_length = cluster_points[:, 0].max() - cluster_points[:, 0].min()\n",
    "                        cluster_height = cluster_points[:, 1].max() - cluster_points[:, 1].min()\n",
    "                        cluster_gradient_tr = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].max() - cluster_points[:, 0].min())\n",
    "                        cluster_gradient_tl = (cluster_points[:, 1].max() - cluster_points[:, 1].min()) / (cluster_points[:, 0].min() - cluster_points[:, 0].max())\n",
    "                        cluster_lengths.append(cluster_length)\n",
    "                        cluster_heights.append(cluster_height)\n",
    "                        cluster_gradient_trs.append(cluster_gradient_tr)\n",
    "                        cluster_gradient_tls.append(cluster_gradient_tl)\n",
    "                else:\n",
    "                    # Fill in zeros for any missing clusters\n",
    "                    cluster_lengths.append(0)\n",
    "                    cluster_heights.append(0)\n",
    "                    cluster_gradient_trs.append(0)\n",
    "                    cluster_gradient_tls.append(0)\n",
    "            clusters.append({'filename': filename, 'cluster_count': cluster_count, 'cluster_lengths': cluster_lengths, 'cluster_heights': cluster_heights, 'cluster_gradient_trs': cluster_gradient_trs, 'cluster_gradient_tls': cluster_gradient_tls})\n",
    "\n",
    "    # Convert the clusters list to a dataframe\n",
    "    clusters_df = pd.DataFrame(clusters)\n",
    "\n",
    "    final_df = clusters_df\n",
    "\n",
    "    print(final_df)\n",
    "\n",
    "    df = final_df\n",
    "    # Expand the list columns into separate columns\n",
    "    expanded_df = pd.concat([df.drop(['cluster_lengths', 'cluster_heights', 'cluster_gradient_trs', 'cluster_gradient_tls'], axis=1),\n",
    "                            df['cluster_lengths'].apply(pd.Series).add_prefix('cluster_length_'),\n",
    "                            df['cluster_heights'].apply(pd.Series).add_prefix('cluster_height_'),\n",
    "                            df['cluster_gradient_trs'].apply(pd.Series).add_prefix('cluster_gradient_tr_'),\n",
    "                            df['cluster_gradient_tls'].apply(pd.Series).add_prefix('cluster_gradient_tl_')],\n",
    "                            axis=1)\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    expanded_df.fillna(0, inplace=True)\n",
    "    print(expanded_df)\n",
    "    return expanded_df\n",
    "\n",
    "def getFeatures(final_df):\n",
    "    features = final_df.columns.tolist()\n",
    "    features.remove('filename')\n",
    "    # features.remove('cluster_count')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to apply the trained model onto any new test dataset, the new test dataset must have the same number of features\n",
    "# meaning that we must apply the HDBScan clustering to the new test data, generate its columns, before finally \n",
    "# loading it into the model using predictions = model.predict(new_data[features])\n",
    "\n",
    "# step 1: load folder containing all testing files into a list of dataframes\n",
    "dirpath = \"test\" # name of folder\n",
    "to_be_predicted_df = load_dataframes(dirpath)\n",
    "\n",
    "# step 2: preprocess the data, which entails removing noise, getting cluster information for comparison, and dataframe conversion to include only the relevant features\n",
    "to_be_predicted_df = preprocess_data(to_be_predicted_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = getFeatures(to_be_predicted_df)\n",
    "to_be_predicted_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X = to_be_predicted_df[f]\n",
    "predictions = DecisionTreeClassifierModel.predict(X)\n",
    "# print(getFeatures(to_be_predicted_df).dtypes)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_df = load_dataframes(dirpath)\n",
    "for i, dataframe in enumerate(placeholder_df):\n",
    "    plt.scatter(dataframe['phase_angle'], dataframe['peak_amplitude'], s=8)\n",
    "    plt.title(f\"{dataframe['filename'].iloc[0]} (Prediction: {predictions[i]})\")\n",
    "    plt.xlabel('Phase angle')\n",
    "    plt.ylabel('Peak amplitude')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
